{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I intend to evaluate the hypothesis that having foreign-born players on a team leads to better FIFA rankings in men’s football. I found two data sources: the first is a list of FIFA rankings from 1992-2019, the second is a list of foreign-born players who have played in FIFA world cups from 1930 until 2018.\n",
    "\n",
    "Summary of wrangling steps performed previously:\n",
    "1. Loaded the first csv file containing FIFA rankings as df1.\n",
    "\n",
    "2. Eventually decided to focus on 3 df1 columns for the final statistical analysis: ‘rank’, ‘rank-date’ renamed to \n",
    "   'date' and ‘country-full’ renamed to 'country'.\n",
    "   \n",
    "3. Loaded the second file containing foreign-born players in FIFA world cup tournaments as df2.\n",
    "\n",
    "4. I decided to focus on four df2 columns for statistical analysis: “NameFootballPlayer”, “'International” \n",
    "   (renamed to \"country\"), “FIFAWorldCup”(which contains the year the player participated in the tournament \n",
    "   therefore renamed to “date”). \n",
    "   \n",
    "5. Performed an inner merge on df1 and df2 on the “country” column in each respective DataFrame. I then \n",
    "   dropped the extra date column from df2.\n",
    "   \n",
    "6. In an effort to minimize the number of rows in the merged dataframe(at this point numbering at about 2 \n",
    "    million) I sliced the rows to only focus on dates from January 1994 onwards.\n",
    "\n",
    "Summary of EDA and Inferential Statistics performed previously:\n",
    "1. The merge process on df1 (about 60,000 rows) and df2 (about 10,000 rows) resulted in an inflated dataset with \n",
    "   over 1 million rows.\n",
    "2. Performed a chi-square test of independence on the 'rank' column and the 'Foreign-born' column which resulted \n",
    "   in a p value of zero and a rejection of the Null hypothesis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataframe saved from data wrangling process\n",
    "new_df = pd.read_csv('wrangled-dataframe', parse_dates = ['date_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to print out rows and columns of dataframe\n",
    "def shape_df(df):\n",
    "    print(\"This dataframe has {r} rows and {c} columns.\".format(r = df.shape[0], c = df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1302239 entries, 0 to 1302238\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count    Dtype         \n",
      "---  ------              --------------    -----         \n",
      " 0   rank                1302239 non-null  int64         \n",
      " 1   country             1302239 non-null  object        \n",
      " 2   NameFootballPlayer  1302239 non-null  object        \n",
      " 3   Foreign-born        1302239 non-null  int64         \n",
      " 4   date_y              1302239 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(2), object(2)\n",
      "memory usage: 49.7+ MB\n"
     ]
    }
   ],
   "source": [
    "cols = ['rank', 'country', 'NameFootballPlayer', 'Foreign-born', 'date_y']\n",
    "new_df = new_df[cols]\n",
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>country</th>\n",
       "      <th>NameFootballPlayer</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>date_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Michel Preud'homme</td>\n",
       "      <td>0</td>\n",
       "      <td>1994-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Dirk Medved</td>\n",
       "      <td>0</td>\n",
       "      <td>1994-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Vital Borkelmans</td>\n",
       "      <td>0</td>\n",
       "      <td>1994-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Philippe Albert</td>\n",
       "      <td>0</td>\n",
       "      <td>1994-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Rudi Smidts</td>\n",
       "      <td>0</td>\n",
       "      <td>1994-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank  country  NameFootballPlayer  Foreign-born     date_y\n",
       "0     1  Belgium  Michel Preud'homme             0 1994-01-01\n",
       "1     1  Belgium         Dirk Medved             0 1994-01-01\n",
       "2     1  Belgium    Vital Borkelmans             0 1994-01-01\n",
       "3     1  Belgium     Philippe Albert             0 1994-01-01\n",
       "4     1  Belgium         Rudi Smidts             0 1994-01-01"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Foreign-born'] = new_df['Foreign-born'].astype(\"category\")\n",
    "new_df['country'] = new_df['country'].astype(\"category\")\n",
    "new_df['NameFootballPlayer'] = new_df['NameFootballPlayer'].astype('object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1302239 entries, 0 to 1302238\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count    Dtype         \n",
      "---  ------              --------------    -----         \n",
      " 0   rank                1302239 non-null  int64         \n",
      " 1   country             1302239 non-null  category      \n",
      " 2   NameFootballPlayer  1302239 non-null  object        \n",
      " 3   Foreign-born        1302239 non-null  category      \n",
      " 4   date_y              1302239 non-null  datetime64[ns]\n",
      "dtypes: category(2), datetime64[ns](1), int64(1), object(1)\n",
      "memory usage: 32.3+ MB\n"
     ]
    }
   ],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df.drop('rank', axis =1)\n",
    "y = new_df['rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder = OneHotEncoder()\n",
    "X = onehotencoder.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1302239, 3516)\n",
      "(1302239,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(911567, 3516)\n",
      "(911567,)\n",
      "(390672, 3516)\n",
      "(390672,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Untuned tree\n",
    "tree = DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.72      0.42     13880\n",
      "           1       0.21      0.47      0.29     12217\n",
      "           2       0.16      0.21      0.18     12149\n",
      "           3       0.10      0.17      0.12     11292\n",
      "           4       0.04      0.02      0.03     11426\n",
      "           5       0.08      0.08      0.08     10994\n",
      "           6       0.08      0.11      0.09     10216\n",
      "           7       0.08      0.12      0.10     10083\n",
      "           8       0.05      0.03      0.03      9938\n",
      "           9       0.05      0.04      0.05      8889\n",
      "          10       0.02      0.01      0.01      8345\n",
      "          11       0.06      0.09      0.07      8523\n",
      "          12       0.04      0.05      0.04      7430\n",
      "          13       0.04      0.05      0.04      7674\n",
      "          14       0.04      0.02      0.02      7803\n",
      "          15       0.04      0.06      0.05      7524\n",
      "          16       0.04      0.09      0.05      8166\n",
      "          17       0.04      0.04      0.04      7261\n",
      "          18       0.01      0.00      0.00      6143\n",
      "          19       0.01      0.00      0.00      6252\n",
      "          20       0.03      0.02      0.02      5991\n",
      "          21       0.02      0.01      0.01      5559\n",
      "          22       0.02      0.01      0.01      5837\n",
      "          23       0.04      0.07      0.05      5972\n",
      "          24       0.02      0.01      0.02      5131\n",
      "          25       0.02      0.02      0.02      5178\n",
      "          26       0.02      0.04      0.03      5268\n",
      "          27       0.02      0.02      0.02      4798\n",
      "          28       0.02      0.03      0.03      4852\n",
      "          29       0.03      0.09      0.04      5270\n",
      "          30       0.03      0.06      0.04      4603\n",
      "          31       0.02      0.03      0.03      4626\n",
      "          32       0.02      0.01      0.01      4901\n",
      "          33       0.01      0.01      0.01      4556\n",
      "          34       0.04      0.08      0.05      4831\n",
      "          35       0.03      0.02      0.02      4868\n",
      "          36       0.04      0.06      0.05      4144\n",
      "          37       0.03      0.05      0.04      4203\n",
      "          38       0.02      0.01      0.01      4185\n",
      "          39       0.05      0.05      0.05      4326\n",
      "          40       0.01      0.00      0.00      3806\n",
      "          41       0.00      0.00      0.00      3846\n",
      "          42       0.01      0.00      0.00      3428\n",
      "          43       0.01      0.00      0.00      3894\n",
      "          44       0.03      0.01      0.02      3575\n",
      "          45       0.05      0.01      0.02      3133\n",
      "          46       0.02      0.00      0.00      3417\n",
      "          47       0.05      0.08      0.06      3279\n",
      "          48       0.04      0.02      0.03      2931\n",
      "          49       0.04      0.08      0.05      3459\n",
      "          50       0.00      0.00      0.00      3105\n",
      "          51       0.02      0.02      0.02      2984\n",
      "          52       0.01      0.01      0.01      2870\n",
      "          53       0.04      0.02      0.03      2647\n",
      "          54       0.03      0.02      0.03      2680\n",
      "          55       0.01      0.01      0.01      2775\n",
      "          56       0.01      0.00      0.00      2553\n",
      "          57       0.02      0.01      0.01      2051\n",
      "          58       0.01      0.00      0.00      2255\n",
      "          59       0.01      0.00      0.00      1966\n",
      "          60       0.02      0.01      0.01      2352\n",
      "          61       0.00      0.00      0.00      2119\n",
      "          62       0.00      0.00      0.00      2035\n",
      "          63       0.00      0.00      0.00      1920\n",
      "          64       0.00      0.00      0.00      2018\n",
      "          65       0.01      0.00      0.00      1694\n",
      "          66       0.01      0.00      0.00      1839\n",
      "          67       0.00      0.00      0.00      1609\n",
      "          68       0.02      0.01      0.01      1628\n",
      "          69       0.03      0.04      0.04      1619\n",
      "          70       0.01      0.01      0.01      1293\n",
      "          71       0.04      0.12      0.06      1292\n",
      "          72       0.00      0.00      0.00      1058\n",
      "          73       0.00      0.00      0.00      1106\n",
      "          74       0.00      0.00      0.00      1223\n",
      "          75       0.00      0.00      0.00      1272\n",
      "          76       0.00      0.00      0.00      1116\n",
      "          77       0.01      0.00      0.01       903\n",
      "          78       0.02      0.01      0.01      1006\n",
      "          79       0.01      0.00      0.00       986\n",
      "          80       0.00      0.00      0.00      1110\n",
      "          81       0.00      0.00      0.00       542\n",
      "          82       0.00      0.00      0.00       702\n",
      "          83       0.00      0.00      0.00       481\n",
      "          84       0.00      0.00      0.00       652\n",
      "          85       0.00      0.00      0.00       456\n",
      "          86       0.01      0.00      0.00       527\n",
      "          87       0.00      0.00      0.00       631\n",
      "          88       0.02      0.01      0.01       678\n",
      "          89       0.01      0.00      0.00       446\n",
      "          90       0.00      0.00      0.00       342\n",
      "          91       0.00      0.00      0.00       533\n",
      "          92       0.00      0.00      0.00       346\n",
      "          93       0.00      0.00      0.00       410\n",
      "          94       0.00      0.00      0.00       485\n",
      "          95       0.00      0.00      0.00       280\n",
      "          96       0.00      0.00      0.00       314\n",
      "          97       0.00      0.00      0.00       310\n",
      "          98       0.00      0.00      0.00       225\n",
      "          99       0.02      0.10      0.03       237\n",
      "         100       0.00      0.00      0.00       346\n",
      "         101       0.00      0.00      0.00       256\n",
      "         102       0.00      0.00      0.00       241\n",
      "         103       0.00      0.00      0.00       123\n",
      "         104       0.00      0.00      0.00       151\n",
      "         105       0.00      0.00      0.00       181\n",
      "         106       0.00      0.00      0.00       158\n",
      "         107       0.00      0.00      0.00       203\n",
      "         108       0.00      0.00      0.00       165\n",
      "         109       0.00      0.00      0.00        49\n",
      "         110       0.01      0.09      0.03       120\n",
      "         111       0.00      0.00      0.00        87\n",
      "         112       0.00      0.00      0.00       129\n",
      "         113       0.00      0.00      0.00        83\n",
      "         114       0.00      0.00      0.00        38\n",
      "         115       0.00      0.00      0.00       127\n",
      "         116       0.00      0.00      0.00        91\n",
      "         117       0.00      0.00      0.00       140\n",
      "         118       0.01      0.09      0.02        91\n",
      "         119       0.00      0.00      0.00       133\n",
      "         120       0.00      0.00      0.00       158\n",
      "         121       0.00      0.00      0.00       224\n",
      "         122       0.00      0.00      0.00       155\n",
      "         123       0.00      0.00      0.00       135\n",
      "         124       0.00      0.00      0.00       109\n",
      "         125       0.00      0.00      0.00        79\n",
      "         126       0.00      0.00      0.00        47\n",
      "         127       0.00      0.00      0.00        84\n",
      "         128       0.00      0.00      0.00        37\n",
      "         129       0.00      0.00      0.00        91\n",
      "         130       0.00      0.00      0.00       144\n",
      "         131       0.00      0.00      0.00        41\n",
      "         132       0.00      0.00      0.00        73\n",
      "         133       0.00      0.00      0.00        79\n",
      "         134       0.00      0.00      0.00        52\n",
      "         135       0.00      0.00      0.00        53\n",
      "         136       0.00      0.00      0.00        25\n",
      "         137       0.00      0.00      0.00        36\n",
      "         138       0.00      0.00      0.00        30\n",
      "         139       0.00      0.00      0.00        37\n",
      "         140       0.00      0.00      0.00        61\n",
      "         141       0.00      0.00      0.00        24\n",
      "         142       0.00      0.00      0.00        16\n",
      "         143       0.00      0.00      0.00        45\n",
      "         144       0.00      0.00      0.00         4\n",
      "         145       0.00      0.00      0.00        10\n",
      "         146       0.00      0.00      0.00        24\n",
      "         147       0.00      0.00      0.00        21\n",
      "         148       0.00      0.00      0.00        33\n",
      "         149       0.00      0.00      0.00         6\n",
      "         150       0.00      0.00      0.00         9\n",
      "         151       0.00      0.00      0.00         4\n",
      "         152       0.00      0.00      0.00         8\n",
      "         153       0.00      0.00      0.00         4\n",
      "         154       0.00      0.00      0.00        18\n",
      "         155       0.00      0.00      0.00         7\n",
      "         156       0.00      0.00      0.00        11\n",
      "         157       0.00      0.00      0.00         7\n",
      "         158       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.08    390672\n",
      "   macro avg       0.02      0.02      0.02    390672\n",
      "weighted avg       0.05      0.08      0.06    390672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.08207140516853012\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score=nan,\n",
       "                   estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features=None,\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    presort='deprecated',\n",
       "                                                    random_state=42,\n",
       "                                                    splitter='best'),\n",
       "                   iid='deprecated', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'criterion': ('gini', 'entropy'),\n",
       "                                        'max_depth': (10, 30),\n",
       "                                        'max_features': ('auto', 'sqrt',\n",
       "                                                         'log2'),\n",
       "                                        'min_samples_split': (2, 4, 6)},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tuning tree\n",
    "parameters = {'max_depth' : (10,30)\n",
    "              , 'criterion' : ('gini', 'entropy')\n",
    "              , 'max_features' : ('auto', 'sqrt', 'log2')\n",
    "              , 'min_samples_split' : (2,4,6)\n",
    "             }\n",
    "\n",
    "DT_grid  = RandomizedSearchCV(DecisionTreeClassifier(random_state = 42), param_distributions = parameters, cv = 5, verbose = True)\n",
    "DT_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=42, splitter='best')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT_grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuned tree\n",
    "tree2 = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "                       max_depth=30, max_features='sqrt', max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "                       random_state=42, splitter='best')\n",
    "\n",
    "tree2.fit(X_train, y_train)\n",
    "y_pred2 = tree2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.72      0.42     13880\n",
      "           1       0.21      0.47      0.29     12217\n",
      "           2       0.16      0.21      0.18     12149\n",
      "           3       0.10      0.17      0.12     11292\n",
      "           4       0.04      0.02      0.03     11426\n",
      "           5       0.08      0.08      0.08     10994\n",
      "           6       0.08      0.11      0.09     10216\n",
      "           7       0.08      0.12      0.10     10083\n",
      "           8       0.05      0.03      0.03      9938\n",
      "           9       0.05      0.04      0.05      8889\n",
      "          10       0.02      0.01      0.01      8345\n",
      "          11       0.06      0.09      0.07      8523\n",
      "          12       0.04      0.05      0.04      7430\n",
      "          13       0.04      0.05      0.04      7674\n",
      "          14       0.04      0.02      0.02      7803\n",
      "          15       0.04      0.06      0.05      7524\n",
      "          16       0.04      0.09      0.05      8166\n",
      "          17       0.04      0.04      0.04      7261\n",
      "          18       0.01      0.00      0.00      6143\n",
      "          19       0.01      0.00      0.00      6252\n",
      "          20       0.03      0.02      0.02      5991\n",
      "          21       0.02      0.01      0.01      5559\n",
      "          22       0.02      0.01      0.01      5837\n",
      "          23       0.04      0.07      0.05      5972\n",
      "          24       0.02      0.01      0.02      5131\n",
      "          25       0.02      0.02      0.02      5178\n",
      "          26       0.02      0.04      0.03      5268\n",
      "          27       0.02      0.02      0.02      4798\n",
      "          28       0.02      0.03      0.03      4852\n",
      "          29       0.03      0.09      0.04      5270\n",
      "          30       0.03      0.06      0.04      4603\n",
      "          31       0.02      0.03      0.03      4626\n",
      "          32       0.02      0.01      0.01      4901\n",
      "          33       0.01      0.01      0.01      4556\n",
      "          34       0.04      0.08      0.05      4831\n",
      "          35       0.03      0.02      0.02      4868\n",
      "          36       0.04      0.06      0.05      4144\n",
      "          37       0.03      0.05      0.04      4203\n",
      "          38       0.02      0.01      0.01      4185\n",
      "          39       0.05      0.05      0.05      4326\n",
      "          40       0.01      0.00      0.00      3806\n",
      "          41       0.00      0.00      0.00      3846\n",
      "          42       0.01      0.00      0.00      3428\n",
      "          43       0.01      0.00      0.00      3894\n",
      "          44       0.03      0.01      0.02      3575\n",
      "          45       0.05      0.01      0.02      3133\n",
      "          46       0.02      0.00      0.00      3417\n",
      "          47       0.05      0.08      0.06      3279\n",
      "          48       0.04      0.02      0.03      2931\n",
      "          49       0.04      0.08      0.05      3459\n",
      "          50       0.00      0.00      0.00      3105\n",
      "          51       0.02      0.02      0.02      2984\n",
      "          52       0.01      0.01      0.01      2870\n",
      "          53       0.04      0.02      0.03      2647\n",
      "          54       0.03      0.02      0.03      2680\n",
      "          55       0.01      0.01      0.01      2775\n",
      "          56       0.01      0.00      0.00      2553\n",
      "          57       0.02      0.01      0.01      2051\n",
      "          58       0.01      0.00      0.00      2255\n",
      "          59       0.01      0.00      0.00      1966\n",
      "          60       0.02      0.01      0.01      2352\n",
      "          61       0.00      0.00      0.00      2119\n",
      "          62       0.00      0.00      0.00      2035\n",
      "          63       0.00      0.00      0.00      1920\n",
      "          64       0.00      0.00      0.00      2018\n",
      "          65       0.01      0.00      0.00      1694\n",
      "          66       0.01      0.00      0.00      1839\n",
      "          67       0.00      0.00      0.00      1609\n",
      "          68       0.02      0.01      0.01      1628\n",
      "          69       0.03      0.04      0.04      1619\n",
      "          70       0.01      0.01      0.01      1293\n",
      "          71       0.04      0.12      0.06      1292\n",
      "          72       0.00      0.00      0.00      1058\n",
      "          73       0.00      0.00      0.00      1106\n",
      "          74       0.00      0.00      0.00      1223\n",
      "          75       0.00      0.00      0.00      1272\n",
      "          76       0.00      0.00      0.00      1116\n",
      "          77       0.01      0.00      0.01       903\n",
      "          78       0.02      0.01      0.01      1006\n",
      "          79       0.01      0.00      0.00       986\n",
      "          80       0.00      0.00      0.00      1110\n",
      "          81       0.00      0.00      0.00       542\n",
      "          82       0.00      0.00      0.00       702\n",
      "          83       0.00      0.00      0.00       481\n",
      "          84       0.00      0.00      0.00       652\n",
      "          85       0.00      0.00      0.00       456\n",
      "          86       0.01      0.00      0.00       527\n",
      "          87       0.00      0.00      0.00       631\n",
      "          88       0.02      0.01      0.01       678\n",
      "          89       0.01      0.00      0.00       446\n",
      "          90       0.00      0.00      0.00       342\n",
      "          91       0.00      0.00      0.00       533\n",
      "          92       0.00      0.00      0.00       346\n",
      "          93       0.00      0.00      0.00       410\n",
      "          94       0.00      0.00      0.00       485\n",
      "          95       0.00      0.00      0.00       280\n",
      "          96       0.00      0.00      0.00       314\n",
      "          97       0.00      0.00      0.00       310\n",
      "          98       0.00      0.00      0.00       225\n",
      "          99       0.02      0.10      0.03       237\n",
      "         100       0.00      0.00      0.00       346\n",
      "         101       0.00      0.00      0.00       256\n",
      "         102       0.00      0.00      0.00       241\n",
      "         103       0.00      0.00      0.00       123\n",
      "         104       0.00      0.00      0.00       151\n",
      "         105       0.00      0.00      0.00       181\n",
      "         106       0.00      0.00      0.00       158\n",
      "         107       0.00      0.00      0.00       203\n",
      "         108       0.00      0.00      0.00       165\n",
      "         109       0.00      0.00      0.00        49\n",
      "         110       0.01      0.09      0.03       120\n",
      "         111       0.00      0.00      0.00        87\n",
      "         112       0.00      0.00      0.00       129\n",
      "         113       0.00      0.00      0.00        83\n",
      "         114       0.00      0.00      0.00        38\n",
      "         115       0.00      0.00      0.00       127\n",
      "         116       0.00      0.00      0.00        91\n",
      "         117       0.00      0.00      0.00       140\n",
      "         118       0.01      0.09      0.02        91\n",
      "         119       0.00      0.00      0.00       133\n",
      "         120       0.00      0.00      0.00       158\n",
      "         121       0.00      0.00      0.00       224\n",
      "         122       0.00      0.00      0.00       155\n",
      "         123       0.00      0.00      0.00       135\n",
      "         124       0.00      0.00      0.00       109\n",
      "         125       0.00      0.00      0.00        79\n",
      "         126       0.00      0.00      0.00        47\n",
      "         127       0.00      0.00      0.00        84\n",
      "         128       0.00      0.00      0.00        37\n",
      "         129       0.00      0.00      0.00        91\n",
      "         130       0.00      0.00      0.00       144\n",
      "         131       0.00      0.00      0.00        41\n",
      "         132       0.00      0.00      0.00        73\n",
      "         133       0.00      0.00      0.00        79\n",
      "         134       0.00      0.00      0.00        52\n",
      "         135       0.00      0.00      0.00        53\n",
      "         136       0.00      0.00      0.00        25\n",
      "         137       0.00      0.00      0.00        36\n",
      "         138       0.00      0.00      0.00        30\n",
      "         139       0.00      0.00      0.00        37\n",
      "         140       0.00      0.00      0.00        61\n",
      "         141       0.00      0.00      0.00        24\n",
      "         142       0.00      0.00      0.00        16\n",
      "         143       0.00      0.00      0.00        45\n",
      "         144       0.00      0.00      0.00         4\n",
      "         145       0.00      0.00      0.00        10\n",
      "         146       0.00      0.00      0.00        24\n",
      "         147       0.00      0.00      0.00        21\n",
      "         148       0.00      0.00      0.00        33\n",
      "         149       0.00      0.00      0.00         6\n",
      "         150       0.00      0.00      0.00         9\n",
      "         151       0.00      0.00      0.00         4\n",
      "         152       0.00      0.00      0.00         8\n",
      "         153       0.00      0.00      0.00         4\n",
      "         154       0.00      0.00      0.00        18\n",
      "         155       0.00      0.00      0.00         7\n",
      "         156       0.00      0.00      0.00        11\n",
      "         157       0.00      0.00      0.00         7\n",
      "         158       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.08    390672\n",
      "   macro avg       0.02      0.02      0.02    390672\n",
      "weighted avg       0.05      0.08      0.06    390672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.06083364049637548\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new tree has an accuracy of 6% compared to the first tree which had an accuracy of 8%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Untuned RandomForest classifier\n",
    "\n",
    "rand = RandomForestClassifier(n_estimators = 10)\n",
    "rand.fit(X_train, y_train)\n",
    "\n",
    "y_pred3 = rand.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.72      0.43     13880\n",
      "           1       0.21      0.47      0.29     12217\n",
      "           2       0.17      0.21      0.19     12149\n",
      "           3       0.10      0.15      0.12     11292\n",
      "           4       0.04      0.03      0.03     11426\n",
      "           5       0.08      0.08      0.08     10994\n",
      "           6       0.08      0.10      0.09     10216\n",
      "           7       0.09      0.13      0.10     10083\n",
      "           8       0.06      0.03      0.04      9938\n",
      "           9       0.05      0.04      0.04      8889\n",
      "          10       0.02      0.01      0.01      8345\n",
      "          11       0.06      0.10      0.07      8523\n",
      "          12       0.04      0.05      0.04      7430\n",
      "          13       0.04      0.05      0.04      7674\n",
      "          14       0.04      0.02      0.03      7803\n",
      "          15       0.04      0.06      0.05      7524\n",
      "          16       0.04      0.09      0.06      8166\n",
      "          17       0.04      0.04      0.04      7261\n",
      "          18       0.02      0.01      0.01      6143\n",
      "          19       0.01      0.00      0.00      6252\n",
      "          20       0.02      0.01      0.02      5991\n",
      "          21       0.02      0.01      0.01      5559\n",
      "          22       0.02      0.01      0.01      5837\n",
      "          23       0.04      0.06      0.05      5972\n",
      "          24       0.02      0.01      0.01      5131\n",
      "          25       0.02      0.02      0.02      5178\n",
      "          26       0.02      0.03      0.03      5268\n",
      "          27       0.02      0.02      0.02      4798\n",
      "          28       0.03      0.04      0.03      4852\n",
      "          29       0.03      0.08      0.04      5270\n",
      "          30       0.03      0.06      0.04      4603\n",
      "          31       0.03      0.03      0.03      4626\n",
      "          32       0.02      0.01      0.02      4901\n",
      "          33       0.02      0.03      0.02      4556\n",
      "          34       0.04      0.07      0.05      4831\n",
      "          35       0.03      0.02      0.02      4868\n",
      "          36       0.04      0.06      0.05      4144\n",
      "          37       0.03      0.05      0.04      4203\n",
      "          38       0.02      0.01      0.01      4185\n",
      "          39       0.04      0.05      0.05      4326\n",
      "          40       0.00      0.00      0.00      3806\n",
      "          41       0.00      0.00      0.00      3846\n",
      "          42       0.01      0.01      0.01      3428\n",
      "          43       0.02      0.00      0.01      3894\n",
      "          44       0.03      0.01      0.02      3575\n",
      "          45       0.03      0.02      0.03      3133\n",
      "          46       0.02      0.00      0.01      3417\n",
      "          47       0.04      0.07      0.06      3279\n",
      "          48       0.03      0.02      0.02      2931\n",
      "          49       0.04      0.11      0.06      3459\n",
      "          50       0.02      0.01      0.01      3105\n",
      "          51       0.02      0.02      0.02      2984\n",
      "          52       0.01      0.01      0.01      2870\n",
      "          53       0.04      0.02      0.02      2647\n",
      "          54       0.03      0.02      0.02      2680\n",
      "          55       0.01      0.01      0.01      2775\n",
      "          56       0.01      0.00      0.00      2553\n",
      "          57       0.02      0.01      0.01      2051\n",
      "          58       0.01      0.00      0.00      2255\n",
      "          59       0.01      0.00      0.00      1966\n",
      "          60       0.02      0.01      0.01      2352\n",
      "          61       0.00      0.00      0.00      2119\n",
      "          62       0.00      0.00      0.00      2035\n",
      "          63       0.01      0.00      0.00      1920\n",
      "          64       0.00      0.00      0.00      2018\n",
      "          65       0.01      0.01      0.01      1694\n",
      "          66       0.01      0.00      0.00      1839\n",
      "          67       0.01      0.00      0.00      1609\n",
      "          68       0.02      0.02      0.02      1628\n",
      "          69       0.04      0.05      0.04      1619\n",
      "          70       0.01      0.01      0.01      1293\n",
      "          71       0.04      0.13      0.06      1292\n",
      "          72       0.01      0.00      0.00      1058\n",
      "          73       0.00      0.00      0.00      1106\n",
      "          74       0.00      0.00      0.00      1223\n",
      "          75       0.01      0.00      0.00      1272\n",
      "          76       0.00      0.00      0.00      1116\n",
      "          77       0.02      0.01      0.01       903\n",
      "          78       0.02      0.01      0.02      1006\n",
      "          79       0.01      0.00      0.00       986\n",
      "          80       0.00      0.00      0.00      1110\n",
      "          81       0.00      0.00      0.00       542\n",
      "          82       0.00      0.00      0.00       702\n",
      "          83       0.00      0.00      0.00       481\n",
      "          84       0.00      0.00      0.00       652\n",
      "          85       0.00      0.00      0.00       456\n",
      "          86       0.01      0.00      0.00       527\n",
      "          87       0.00      0.00      0.00       631\n",
      "          88       0.02      0.01      0.01       678\n",
      "          89       0.00      0.00      0.00       446\n",
      "          90       0.00      0.00      0.00       342\n",
      "          91       0.00      0.00      0.00       533\n",
      "          92       0.00      0.00      0.00       346\n",
      "          93       0.00      0.00      0.00       410\n",
      "          94       0.00      0.00      0.00       485\n",
      "          95       0.00      0.00      0.00       280\n",
      "          96       0.00      0.00      0.00       314\n",
      "          97       0.00      0.00      0.00       310\n",
      "          98       0.00      0.00      0.00       225\n",
      "          99       0.02      0.07      0.03       237\n",
      "         100       0.00      0.00      0.00       346\n",
      "         101       0.00      0.00      0.00       256\n",
      "         102       0.00      0.00      0.00       241\n",
      "         103       0.00      0.00      0.00       123\n",
      "         104       0.00      0.00      0.00       151\n",
      "         105       0.00      0.00      0.00       181\n",
      "         106       0.00      0.00      0.00       158\n",
      "         107       0.00      0.00      0.00       203\n",
      "         108       0.00      0.00      0.00       165\n",
      "         109       0.00      0.00      0.00        49\n",
      "         110       0.01      0.07      0.02       120\n",
      "         111       0.00      0.00      0.00        87\n",
      "         112       0.00      0.00      0.00       129\n",
      "         113       0.00      0.00      0.00        83\n",
      "         114       0.00      0.00      0.00        38\n",
      "         115       0.00      0.00      0.00       127\n",
      "         116       0.00      0.00      0.00        91\n",
      "         117       0.00      0.00      0.00       140\n",
      "         118       0.02      0.18      0.03        91\n",
      "         119       0.01      0.01      0.01       133\n",
      "         120       0.00      0.00      0.00       158\n",
      "         121       0.02      0.02      0.02       224\n",
      "         122       0.01      0.01      0.01       155\n",
      "         123       0.00      0.00      0.00       135\n",
      "         124       0.01      0.01      0.01       109\n",
      "         125       0.00      0.00      0.00        79\n",
      "         126       0.00      0.00      0.00        47\n",
      "         127       0.00      0.00      0.00        84\n",
      "         128       0.00      0.00      0.00        37\n",
      "         129       0.00      0.00      0.00        91\n",
      "         130       0.00      0.00      0.00       144\n",
      "         131       0.00      0.00      0.00        41\n",
      "         132       0.00      0.00      0.00        73\n",
      "         133       0.00      0.00      0.00        79\n",
      "         134       0.00      0.00      0.00        52\n",
      "         135       0.00      0.00      0.00        53\n",
      "         136       0.00      0.00      0.00        25\n",
      "         137       0.00      0.00      0.00        36\n",
      "         138       0.00      0.00      0.00        30\n",
      "         139       0.00      0.00      0.00        37\n",
      "         140       0.00      0.00      0.00        61\n",
      "         141       0.00      0.00      0.00        24\n",
      "         142       0.00      0.00      0.00        16\n",
      "         143       0.00      0.00      0.00        45\n",
      "         144       0.00      0.00      0.00         4\n",
      "         145       0.00      0.00      0.00        10\n",
      "         146       0.00      0.00      0.00        24\n",
      "         147       0.00      0.00      0.00        21\n",
      "         148       0.00      0.00      0.00        33\n",
      "         149       0.00      0.00      0.00         6\n",
      "         150       0.00      0.00      0.00         9\n",
      "         151       0.00      0.00      0.00         4\n",
      "         152       0.00      0.00      0.00         8\n",
      "         153       0.00      0.00      0.00         4\n",
      "         154       0.00      0.00      0.00        18\n",
      "         155       0.00      0.00      0.00         7\n",
      "         156       0.00      0.00      0.00        11\n",
      "         157       0.00      0.00      0.00         7\n",
      "         158       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.08    390672\n",
      "   macro avg       0.02      0.03      0.02    390672\n",
      "weighted avg       0.05      0.08      0.06    390672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.08237344882663718\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score of the untuned random forest classifier is 8.2% which is similar to the first untuned decision tree classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning the forest\n",
    "n_estimators = np.arange(15, 30)\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [10, 18, 22]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "\n",
    "params = {'n_estimators' : n_estimators\n",
    "            ,'max_depth' : max_depth\n",
    "              , 'max_features' : max_features\n",
    "              , 'min_samples_split' : min_samples_split\n",
    "              , 'min_samples_leaf' : min_samples_leaf\n",
    "              , 'bootstrap': bootstrap\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 34.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs...\n",
       "                   iid='deprecated', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 18, 22],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': array([15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "rand_grid = RandomizedSearchCV(rfc, param_distributions = params, cv = 5, verbose = True)\n",
    "rand_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=22, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=27,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand1 = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='gini', max_depth=18, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=2, min_samples_split=5,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=17,\n",
    "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
    "                       warm_start=False)\n",
    "\n",
    "rand1.fit(X_train, y_train)\n",
    "y_pred4 = rand1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.74      0.11     13880\n",
      "           1       0.15      0.47      0.23     12217\n",
      "           2       0.17      0.21      0.19     12149\n",
      "           3       0.12      0.15      0.13     11292\n",
      "           4       0.04      0.05      0.05     11426\n",
      "           5       0.10      0.11      0.10     10994\n",
      "           6       0.08      0.14      0.10     10216\n",
      "           7       0.10      0.21      0.13     10083\n",
      "           8       0.06      0.05      0.06      9938\n",
      "           9       0.06      0.05      0.05      8889\n",
      "          10       0.00      0.00      0.00      8345\n",
      "          11       0.07      0.09      0.08      8523\n",
      "          12       0.05      0.02      0.03      7430\n",
      "          13       0.05      0.02      0.03      7674\n",
      "          14       0.04      0.00      0.00      7803\n",
      "          15       0.06      0.04      0.05      7524\n",
      "          16       0.05      0.20      0.09      8166\n",
      "          17       0.05      0.01      0.01      7261\n",
      "          18       0.01      0.00      0.00      6143\n",
      "          19       0.00      0.00      0.00      6252\n",
      "          20       0.00      0.00      0.00      5991\n",
      "          21       0.00      0.00      0.00      5559\n",
      "          22       0.00      0.00      0.00      5837\n",
      "          23       0.06      0.07      0.07      5972\n",
      "          24       0.03      0.00      0.01      5131\n",
      "          25       0.04      0.01      0.01      5178\n",
      "          26       0.05      0.01      0.01      5268\n",
      "          27       0.05      0.01      0.01      4798\n",
      "          28       0.00      0.00      0.00      4852\n",
      "          29       0.02      0.00      0.00      5270\n",
      "          30       0.03      0.00      0.00      4603\n",
      "          31       0.02      0.00      0.00      4626\n",
      "          32       0.00      0.00      0.00      4901\n",
      "          33       0.00      0.00      0.00      4556\n",
      "          34       0.00      0.00      0.00      4831\n",
      "          35       0.00      0.00      0.00      4868\n",
      "          36       0.00      0.00      0.00      4144\n",
      "          37       0.00      0.00      0.00      4203\n",
      "          38       0.00      0.00      0.00      4185\n",
      "          39       0.06      0.06      0.06      4326\n",
      "          40       0.00      0.00      0.00      3806\n",
      "          41       0.00      0.00      0.00      3846\n",
      "          42       0.00      0.00      0.00      3428\n",
      "          43       0.00      0.00      0.00      3894\n",
      "          44       0.00      0.00      0.00      3575\n",
      "          45       0.00      0.00      0.00      3133\n",
      "          46       0.00      0.00      0.00      3417\n",
      "          47       0.06      0.08      0.07      3279\n",
      "          48       0.00      0.00      0.00      2931\n",
      "          49       0.04      0.02      0.03      3459\n",
      "          50       0.00      0.00      0.00      3105\n",
      "          51       0.00      0.00      0.00      2984\n",
      "          52       0.00      0.00      0.00      2870\n",
      "          53       0.00      0.00      0.00      2647\n",
      "          54       0.00      0.00      0.00      2680\n",
      "          55       0.00      0.00      0.00      2775\n",
      "          56       0.00      0.00      0.00      2553\n",
      "          57       0.00      0.00      0.00      2051\n",
      "          58       0.00      0.00      0.00      2255\n",
      "          59       0.00      0.00      0.00      1966\n",
      "          60       0.00      0.00      0.00      2352\n",
      "          61       0.00      0.00      0.00      2119\n",
      "          62       0.00      0.00      0.00      2035\n",
      "          63       0.00      0.00      0.00      1920\n",
      "          64       0.00      0.00      0.00      2018\n",
      "          65       0.00      0.00      0.00      1694\n",
      "          66       0.00      0.00      0.00      1839\n",
      "          67       0.00      0.00      0.00      1609\n",
      "          68       0.00      0.00      0.00      1628\n",
      "          69       0.00      0.00      0.00      1619\n",
      "          70       0.00      0.00      0.00      1293\n",
      "          71       0.00      0.00      0.00      1292\n",
      "          72       0.00      0.00      0.00      1058\n",
      "          73       0.00      0.00      0.00      1106\n",
      "          74       0.00      0.00      0.00      1223\n",
      "          75       0.00      0.00      0.00      1272\n",
      "          76       0.00      0.00      0.00      1116\n",
      "          77       0.00      0.00      0.00       903\n",
      "          78       0.00      0.00      0.00      1006\n",
      "          79       0.00      0.00      0.00       986\n",
      "          80       0.00      0.00      0.00      1110\n",
      "          81       0.00      0.00      0.00       542\n",
      "          82       0.00      0.00      0.00       702\n",
      "          83       0.00      0.00      0.00       481\n",
      "          84       0.00      0.00      0.00       652\n",
      "          85       0.00      0.00      0.00       456\n",
      "          86       0.00      0.00      0.00       527\n",
      "          87       0.00      0.00      0.00       631\n",
      "          88       0.00      0.00      0.00       678\n",
      "          89       0.00      0.00      0.00       446\n",
      "          90       0.00      0.00      0.00       342\n",
      "          91       0.00      0.00      0.00       533\n",
      "          92       0.00      0.00      0.00       346\n",
      "          93       0.00      0.00      0.00       410\n",
      "          94       0.00      0.00      0.00       485\n",
      "          95       0.00      0.00      0.00       280\n",
      "          96       0.00      0.00      0.00       314\n",
      "          97       0.00      0.00      0.00       310\n",
      "          98       0.00      0.00      0.00       225\n",
      "          99       0.00      0.00      0.00       237\n",
      "         100       0.00      0.00      0.00       346\n",
      "         101       0.00      0.00      0.00       256\n",
      "         102       0.00      0.00      0.00       241\n",
      "         103       0.00      0.00      0.00       123\n",
      "         104       0.00      0.00      0.00       151\n",
      "         105       0.00      0.00      0.00       181\n",
      "         106       0.00      0.00      0.00       158\n",
      "         107       0.00      0.00      0.00       203\n",
      "         108       0.00      0.00      0.00       165\n",
      "         109       0.00      0.00      0.00        49\n",
      "         110       0.00      0.00      0.00       120\n",
      "         111       0.00      0.00      0.00        87\n",
      "         112       0.00      0.00      0.00       129\n",
      "         113       0.00      0.00      0.00        83\n",
      "         114       0.00      0.00      0.00        38\n",
      "         115       0.00      0.00      0.00       127\n",
      "         116       0.00      0.00      0.00        91\n",
      "         117       0.00      0.00      0.00       140\n",
      "         118       0.00      0.00      0.00        91\n",
      "         119       0.00      0.00      0.00       133\n",
      "         120       0.00      0.00      0.00       158\n",
      "         121       0.00      0.00      0.00       224\n",
      "         122       0.00      0.00      0.00       155\n",
      "         123       0.00      0.00      0.00       135\n",
      "         124       0.00      0.00      0.00       109\n",
      "         125       0.00      0.00      0.00        79\n",
      "         126       0.00      0.00      0.00        47\n",
      "         127       0.00      0.00      0.00        84\n",
      "         128       0.00      0.00      0.00        37\n",
      "         129       0.00      0.00      0.00        91\n",
      "         130       0.00      0.00      0.00       144\n",
      "         131       0.00      0.00      0.00        41\n",
      "         132       0.00      0.00      0.00        73\n",
      "         133       0.00      0.00      0.00        79\n",
      "         134       0.00      0.00      0.00        52\n",
      "         135       0.00      0.00      0.00        53\n",
      "         136       0.00      0.00      0.00        25\n",
      "         137       0.00      0.00      0.00        36\n",
      "         138       0.00      0.00      0.00        30\n",
      "         139       0.00      0.00      0.00        37\n",
      "         140       0.00      0.00      0.00        61\n",
      "         141       0.00      0.00      0.00        24\n",
      "         142       0.00      0.00      0.00        16\n",
      "         143       0.00      0.00      0.00        45\n",
      "         144       0.00      0.00      0.00         4\n",
      "         145       0.00      0.00      0.00        10\n",
      "         146       0.00      0.00      0.00        24\n",
      "         147       0.00      0.00      0.00        21\n",
      "         148       0.00      0.00      0.00        33\n",
      "         149       0.00      0.00      0.00         6\n",
      "         150       0.00      0.00      0.00         9\n",
      "         151       0.00      0.00      0.00         4\n",
      "         152       0.00      0.00      0.00         8\n",
      "         153       0.00      0.00      0.00         4\n",
      "         154       0.00      0.00      0.00        18\n",
      "         155       0.00      0.00      0.00         7\n",
      "         156       0.00      0.00      0.00        11\n",
      "         157       0.00      0.00      0.00         7\n",
      "         158       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.08    390672\n",
      "   macro avg       0.01      0.02      0.01    390672\n",
      "weighted avg       0.04      0.08      0.04    390672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0788666707621739\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=5)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred5 = clf.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08478467870745791"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project was to evaluate the hypothesis that having foreign-born players on a team leads to better FIFA rankings in men’s football. Running the chi-square test of independence suggested indeed that the two might be dependent. This might have been influenced by an inflated data set that arose from the merging process of two data frames. Nevertheless, I decided to train several classifier models to test my hypothesis, namely: Decision Tree, Random Forest and Gradient Boosting Classifier. The goal was to see if these models could predict the rank of a country based on the number of foreign-born players on its team. All three classifiers performed poorly with a score of around 8%. This reinforced the need for a robust number of features given that the dataset I trained on only had one feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Feature engineering to bolster the number of features in the dataset\n",
    "2. Augmenting the date-time column to potentially run a timeseries\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
